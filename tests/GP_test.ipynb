{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DagmaDCE import utils, nonlinear, nonlinear_dce\n",
    "import torch, gpytorch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from CausalDisco.analytics import r2_sortability, var_sortability\n",
    "from CausalDisco.baselines import r2_sort_regress, var_sort_regress\n",
    "from cdt.metrics import SID\n",
    "from scipy.stats import kendalltau\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "utils.set_random_seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "reestimate_graph = False\n",
    "RESULTS_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating Data <<<\n",
      "[Var Sort Regress Results] Var-Sortability of X: 0.7142857142857143\n",
      "[Var Sort Regress Results] SHD: 24 | SID: 43.0 | F1: 0.45614035087719296\n",
      "[R^2 Sort Regress Results] R^2-Sortability of X: 1.0\n",
      "[R^2 Sort Regress Results] SHD: 0 | SID: 0.0 | F1: 1.0\n",
      "X shape: torch.Size([1000, 10])\n",
      "B_true shape: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "print('>>> Generating Data <<<')\n",
    "n, d, s0, graph_type, sem_type = 1000, 10, 20, 'ER', 'gauss'\n",
    "B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "W_true = utils.simulate_parameter(B_true)\n",
    "X = utils.simulate_linear_sem(W_true, n, sem_type)\n",
    "\n",
    "results_r2_sort_regress = r2_sort_regress(X)\n",
    "acc_r2_sort_regress = utils.count_accuracy(\n",
    "    B_true, results_r2_sort_regress != 0)\n",
    "sid_r2_sort_regress = SID(B_true, results_r2_sort_regress != 0).item()\n",
    "print('[Var Sort Regress Results] Var-Sortability of X:',\n",
    "      r2_sortability(X, W_true))\n",
    "print('[Var Sort Regress Results] SHD:',\n",
    "      acc_r2_sort_regress['shd'], '| SID:', sid_r2_sort_regress, '| F1:', acc_r2_sort_regress['f1'])\n",
    "\n",
    "results_var_sort_regress = var_sort_regress(X)\n",
    "acc_var_sort_regress = utils.count_accuracy(\n",
    "    B_true, results_var_sort_regress != 0)\n",
    "sid_var_sort_regress = SID(B_true, results_var_sort_regress != 0).item()\n",
    "print('[R^2 Sort Regress Results] R^2-Sortability of X:',\n",
    "      var_sortability(X, W_true))\n",
    "print('[R^2 Sort Regress Results] SHD:',\n",
    "      acc_var_sort_regress['shd'], '| SID:', sid_var_sort_regress, '| F1:', acc_var_sort_regress['f1'])\n",
    "\n",
    "X = torch.from_numpy(X).to(device)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"B_true shape: {B_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import abc\n",
    "import typing\n",
    "import gpytorch\n",
    "from gpytorch.mlls import SumMarginalLogLikelihood\n",
    "\n",
    "class Dagma_DCE_Module(nn.Module, abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get_graph(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def h_func(self, W: torch.Tensor, s: float) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_l1_reg(self, W: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class DagmaDCE:\n",
    "    def __init__(self, model: Dagma_DCE_Module, use_mse_loss=True):\n",
    "        \"\"\"Initializes a DAGMA DCE model. Requires a `DAGMA_DCE_Module`\n",
    "\n",
    "        Args:\n",
    "            model (Dagma_DCE_Module): module implementing adjacency matrix,\n",
    "                h_func constraint, and L1 regularization\n",
    "            use_mse_loss (bool, optional): to use MSE loss instead of log MSE loss.\n",
    "                Defaults to True.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss = self.mse_loss if use_mse_loss else self.log_mse_loss\n",
    "\n",
    "\n",
    "    def mse_loss(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Computes the MSE loss sum (output - target)^2 / (2N)\"\"\"\n",
    "        n, d = target.shape\n",
    "        print(type(output))\n",
    "        if isinstance(output, torch.distributions.MultivariateNormal):\n",
    "            output_mean = output.mean\n",
    "        else:\n",
    "            output_mean = output\n",
    "        print(f\"output's type is: {type(output)}\")\n",
    "        print(f\"target's type is: {type(target)}\")\n",
    "        return 0.5 / n * torch.sum((output_mean - target) ** 2)\n",
    "\n",
    "\n",
    "    def log_mse_loss(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Computes the MSE loss d / 2 * log [sum (output - target)^2 / N ]\"\"\"\n",
    "        n, d = target.shape\n",
    "        loss = 0.5 * d * torch.log(1 / n * torch.sum((output - target) ** 2))\n",
    "        return loss\n",
    "\n",
    "    def minimize(\n",
    "        self,\n",
    "        max_iter: int,\n",
    "        lr: float,\n",
    "        lambda1: float,\n",
    "        lambda2: float,\n",
    "        mu: float,\n",
    "        s: float,\n",
    "        pbar: tqdm,\n",
    "        lr_decay: bool = False,\n",
    "        checkpoint: int = 1000,\n",
    "        tol: float = 1e-3,\n",
    "    ):\n",
    "        \"\"\"Perform minimization using the barrier method optimization\n",
    "\n",
    "        Args:\n",
    "            max_iter (int): maximum number of iterations to optimize\n",
    "            lr (float): learning rate for adam\n",
    "            lambda1 (float): regularization parameter\n",
    "            lambda2 (float): weight decay\n",
    "            mu (float): regularization parameter for barrier method\n",
    "            s (float): DAMGA constraint hyperparameter\n",
    "            pbar (tqdm): progress bar to use\n",
    "            lr_decay (bool, optional): whether or not to use learning rate decay.\n",
    "                Defaults to False.\n",
    "            checkpoint (int, optional): how often to checkpoint. Defaults to 1000.\n",
    "            tol (float, optional): tolerance to terminate learning. Defaults to 1e-3.\n",
    "        \"\"\"\n",
    "        print(f\"model parameters are:\")\n",
    "        for param in self.model.parameters():\n",
    "            print(param)\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(0.99, 0.999),\n",
    "            weight_decay=mu * lambda2,\n",
    "        )\n",
    "\n",
    "        trainable_params = sum(\n",
    "            p.numel() for p in self.model.parameters() if p.requires_grad\n",
    "        )\n",
    "\n",
    "        print(f\"total params is: {trainable_params}\")\n",
    "        obj_prev = 1e16\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=0.8 if lr_decay else 1.0\n",
    "        )\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i == 0:\n",
    "                # X_hat = self.model(self.X)\n",
    "                print(self.model(self.X))\n",
    "                print(self.model(self.X).shape)\n",
    "                # print(f\"shape of self.model(self.X) is: {self.model(self.X)}\")\n",
    "                X_hat = self.model(self.X).mean  # Extract mean from MultivariateNormal\n",
    "                score = self.loss(X_hat, self.X)\n",
    "                obj = score\n",
    "\n",
    "            else:\n",
    "                W_current, observed_derivs = self.model.get_graph(self.X)\n",
    "                h_val = self.model.h_func(W_current, s)\n",
    "\n",
    "                if h_val.item() < 0:\n",
    "                    return False\n",
    "                # print(self.model(self.X))\n",
    "                # X_hat = self.model(self.X)\n",
    "                X_hat = self.model(self.X).mean  # Extract mean from MultivariateNormal\n",
    "                score = self.mse_loss(X_hat, self.X)\n",
    "\n",
    "                l1_reg = lambda1 * self.model.get_l1_reg(observed_derivs)\n",
    "\n",
    "                obj = mu * (score + l1_reg) + h_val\n",
    "\n",
    "            obj.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_decay and (i + 1) % 1000 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            if i % checkpoint == 0 or i == max_iter - 1:\n",
    "                obj_new = obj.item()\n",
    "\n",
    "                if np.abs((obj_prev - obj_new) / (obj_prev)) <= tol:\n",
    "                    pbar.update(max_iter - i)\n",
    "                    break\n",
    "                obj_prev = obj_new\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        lambda1: float = 0.02,\n",
    "        lambda2: float = 0.005,\n",
    "        T: int = 4,\n",
    "        mu_init: float = 1.0,\n",
    "        mu_factor: float = 0.1,\n",
    "        s: float = 1.0,\n",
    "        warm_iter: int = 5e3,\n",
    "        max_iter: int = 8e3,\n",
    "        lr: float = 1e-3,\n",
    "        disable_pbar: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Fits the DAGMA-DCE model\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): inputs\n",
    "            lambda1 (float, optional): regularization parameter. Defaults to 0.02.\n",
    "            lambda2 (float, optional): weight decay. Defaults to 0.005.\n",
    "            T (int, optional): number of barrier loops. Defaults to 4.\n",
    "            mu_init (float, optional): barrier path coefficient. Defaults to 1.0.\n",
    "            mu_factor (float, optional): decay parameter for mu. Defaults to 0.1.\n",
    "            s (float, optional): DAGMA constraint hyperparameter. Defaults to 1.0.\n",
    "            warm_iter (int, optional): number of warmup models. Defaults to 5e3.\n",
    "            max_iter (int, optional): maximum number of iterations for learning. Defaults to 8e3.\n",
    "            lr (float, optional): learning rate. Defaults to 1e-3.\n",
    "            disable_pbar (bool, optional): whether or not to use the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: graph returned by the model\n",
    "        \"\"\"\n",
    "        mu = mu_init\n",
    "        self.X = X\n",
    "\n",
    "        with tqdm(total=(T - 1) * warm_iter + max_iter, disable=disable_pbar) as pbar:\n",
    "            for i in range(int(T)):\n",
    "                success, s_cur = False, s\n",
    "                lr_decay = False\n",
    "\n",
    "                inner_iter = int(max_iter) if i == T - 1 else int(warm_iter)\n",
    "                model_copy = copy.deepcopy(self.model)\n",
    "\n",
    "                while success is False:\n",
    "                    success = self.minimize(\n",
    "                        inner_iter,\n",
    "                        lr,\n",
    "                        lambda1,\n",
    "                        lambda2,\n",
    "                        mu,\n",
    "                        s_cur,\n",
    "                        lr_decay=lr_decay,\n",
    "                        pbar=pbar,\n",
    "                    )\n",
    "\n",
    "                    if success is False:\n",
    "                        self.model.load_state_dict(model_copy.state_dict().copy())\n",
    "                        lr *= 0.5\n",
    "                        lr_decay = True\n",
    "                        if lr < 1e-10:\n",
    "                            print(\":(\")\n",
    "                            break  # lr is too small\n",
    "\n",
    "                    mu *= mu_factor\n",
    "                    print(f\"Success is {success}\")\n",
    "\n",
    "        return self.model.get_graph(self.X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DagmaGP_DCE(Dagma_DCE_Module):\n",
    "    def __init__(self, train_x, num_tasks, lr=0.1, training_iterations=50):\n",
    "        super(DagmaGP_DCE, self).__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "        self.lr = lr\n",
    "        self.training_iterations = training_iterations\n",
    "\n",
    "        self.models = []\n",
    "        self.likelihoods = []\n",
    "        for i in range(self.num_tasks):\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "            model = ExactGPModel(train_x, train_x[:, i], likelihood)\n",
    "            self.models.append(model)\n",
    "            self.likelihoods.append(likelihood)\n",
    "\n",
    "        self.model_list = gpytorch.models.IndependentModelList(*self.models)\n",
    "        self.likelihood_list = gpytorch.likelihoods.LikelihoodList(*self.likelihoods)\n",
    "        self.mll = SumMarginalLogLikelihood(self.likelihood_list, self.model_list)\n",
    "\n",
    "        print(\"Initial Lengthscales:\")\n",
    "        for i, model in enumerate(self.model_list.models):\n",
    "            lengthscale = model.covar_module.base_kernel.lengthscale\n",
    "            print(f\"Model {i} initial lengthscale: {lengthscale}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for model in self.model_list.models:\n",
    "            model.eval()\n",
    "\n",
    "        predictive_means = []\n",
    "        predictive_variances = []\n",
    "        for model, likelihood in zip(self.model_list.models, self.likelihood_list.likelihoods):\n",
    "            with torch.no_grad():\n",
    "                observed_pred = likelihood(model(x))\n",
    "                predictive_means.append(observed_pred.mean)\n",
    "                predictive_variances.append(observed_pred.variance)\n",
    "\n",
    "        return torch.stack(predictive_means)\n",
    "\n",
    "    def train(self, train_x):\n",
    "\n",
    "        self.model_list.train()\n",
    "        self.likelihood_list.train()\n",
    "        optimizer = torch.optim.Adam(self.model_list.parameters(), lr=self.lr)\n",
    "\n",
    "        for i in range(self.training_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model_list(*[train_x for _ in range(self.num_tasks)])\n",
    "            targets = [train_x[:, j] for j in range(self.num_tasks)]\n",
    "            loss = -self.mll(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Iter %d/%d - Loss: %.3f' % (i + 1, self.training_iterations, loss.item()))\n",
    "        \n",
    "        print(\"Final Lengthscales:\")\n",
    "        for i, model in enumerate(self.model_list.models):\n",
    "            lengthscale = model.covar_module.base_kernel.lengthscale\n",
    "            print(f\"Model {i} initial lengthscale: {lengthscale}\")\n",
    "\n",
    "    def get_graph(self, x):\n",
    "        for model in self.model_list.models:\n",
    "            model.eval()\n",
    "\n",
    "        derivative = torch.zeros(len(x), self.num_tasks, 10)\n",
    "        for i in range(len(x)):\n",
    "            print(f\"Computing Jacobian for data point {i}\")\n",
    "            input_vector = x[i].unsqueeze(0).detach().requires_grad_(True)\n",
    "            for j, model in enumerate(self.model_list.models):\n",
    "                model.zero_grad()\n",
    "                observed_pred = self.likelihoods[j](model(input_vector))\n",
    "                mean = observed_pred.mean\n",
    "                mean.backward()\n",
    "                derivative[i, j] = input_vector.grad.clone()\n",
    "                input_vector.grad.zero_()\n",
    "\n",
    "        mean_squared_jacobians = torch.mean(derivative ** 2, dim=0)\n",
    "        W = torch.sqrt(mean_squared_jacobians)\n",
    "        return W, derivative\n",
    "\n",
    "    def h_func(self, W: torch.Tensor, s: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"Calculate the DAGMA constraint function\n",
    "\n",
    "        Args:\n",
    "            W (torch.Tensor): adjacency matrix\n",
    "            s (float, optional): hyperparameter for the DAGMA constraint,\n",
    "                can be any positive number. Defaults to 1.0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: constraint\n",
    "        \"\"\"\n",
    "        h = -torch.slogdet(s * self.I - W * W)[1] + self.d * np.log(s)\n",
    "        return h\n",
    "\n",
    "    def get_l1_reg(self, observed_derivs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Gets the L1 regularization\n",
    "\n",
    "        Args:\n",
    "            observed_derivs (torch.Tensor): the batched Jacobian matrix\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        return torch.sum(torch.abs(torch.mean(observed_derivs, axis=0)))\n",
    "    \n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Lengthscales:\n",
      "Model 0 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 1 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 2 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 3 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 4 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 5 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 6 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 7 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 8 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "Model 9 initial lengthscale: tensor([[0.6931]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "eq_model = DagmaGP_DCE(train_x=X, num_tasks = 10, lr=0.1, training_iterations=50)\n",
    "model = DagmaDCE(eq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 3.394\n",
      "Iter 2/50 - Loss: 3.248\n",
      "Iter 3/50 - Loss: 3.109\n",
      "Iter 4/50 - Loss: 2.976\n",
      "Iter 5/50 - Loss: 2.850\n",
      "Iter 6/50 - Loss: 2.727\n",
      "Iter 7/50 - Loss: 2.607\n",
      "Iter 8/50 - Loss: 2.491\n",
      "Iter 9/50 - Loss: 2.377\n",
      "Iter 10/50 - Loss: 2.269\n",
      "Iter 11/50 - Loss: 2.164\n",
      "Iter 12/50 - Loss: 2.069\n",
      "Iter 13/50 - Loss: 1.975\n",
      "Iter 14/50 - Loss: 1.889\n",
      "Iter 15/50 - Loss: 1.810\n",
      "Iter 16/50 - Loss: 1.736\n",
      "Iter 17/50 - Loss: 1.669\n",
      "Iter 18/50 - Loss: 1.605\n",
      "Iter 19/50 - Loss: 1.544\n",
      "Iter 20/50 - Loss: 1.489\n",
      "Iter 21/50 - Loss: 1.435\n",
      "Iter 22/50 - Loss: 1.381\n",
      "Iter 23/50 - Loss: 1.335\n",
      "Iter 24/50 - Loss: 1.286\n",
      "Iter 25/50 - Loss: 1.242\n",
      "Iter 26/50 - Loss: 1.197\n",
      "Iter 27/50 - Loss: 1.155\n",
      "Iter 28/50 - Loss: 1.116\n",
      "Iter 29/50 - Loss: 1.077\n",
      "Iter 30/50 - Loss: 1.029\n",
      "Iter 31/50 - Loss: 0.992\n",
      "Iter 32/50 - Loss: 0.952\n",
      "Iter 33/50 - Loss: 0.917\n",
      "Iter 34/50 - Loss: 0.879\n",
      "Iter 35/50 - Loss: 0.836\n",
      "Iter 36/50 - Loss: 0.801\n",
      "Iter 37/50 - Loss: 0.761\n",
      "Iter 38/50 - Loss: 0.724\n",
      "Iter 39/50 - Loss: 0.693\n",
      "Iter 40/50 - Loss: 0.654\n",
      "Iter 41/50 - Loss: 0.614\n",
      "Iter 42/50 - Loss: 0.581\n",
      "Iter 43/50 - Loss: 0.546\n",
      "Iter 44/50 - Loss: 0.507\n",
      "Iter 45/50 - Loss: 0.478\n",
      "Iter 46/50 - Loss: 0.445\n",
      "Iter 47/50 - Loss: 0.408\n",
      "Iter 48/50 - Loss: 0.372\n",
      "Iter 49/50 - Loss: 0.341\n",
      "Iter 50/50 - Loss: 0.313\n",
      "Final Lengthscales:\n",
      "Model 0 initial lengthscale: tensor([[4.5569]], grad_fn=<SoftplusBackward0>)\n",
      "Model 1 initial lengthscale: tensor([[4.3890]], grad_fn=<SoftplusBackward0>)\n",
      "Model 2 initial lengthscale: tensor([[3.6299]], grad_fn=<SoftplusBackward0>)\n",
      "Model 3 initial lengthscale: tensor([[3.6432]], grad_fn=<SoftplusBackward0>)\n",
      "Model 4 initial lengthscale: tensor([[5.1424]], grad_fn=<SoftplusBackward0>)\n",
      "Model 5 initial lengthscale: tensor([[4.2975]], grad_fn=<SoftplusBackward0>)\n",
      "Model 6 initial lengthscale: tensor([[5.1002]], grad_fn=<SoftplusBackward0>)\n",
      "Model 7 initial lengthscale: tensor([[5.1587]], grad_fn=<SoftplusBackward0>)\n",
      "Model 8 initial lengthscale: tensor([[5.2352]], grad_fn=<SoftplusBackward0>)\n",
      "Model 9 initial lengthscale: tensor([[3.8829]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "eq_model.train(train_x=X)\n",
    "\n",
    "# W_est_dce = model.fit(X, lambda1=0, lambda2=5e-3,\n",
    "#                       lr=2e-4, mu_factor=0.1, mu_init=1, T=4, warm_iter=1*5000, max_iter=1*7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_jacobian, J = eq_model.get_graph(X)\n",
    "print(f\"rms_jacobian is: {rms_jacobian}\")\n",
    "print(f\"J is: {J}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
